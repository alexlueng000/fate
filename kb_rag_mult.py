#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, argparse, numpy as np, glob
from typing import List, Tuple, Dict

# ====== å¯é€‰åç«¯ï¼šSentence-Transformersï¼ˆä¼˜å…ˆï¼‰æˆ– TF-IDF ======
USE_ST = False
try:
    from sentence_transformers import SentenceTransformer
    USE_ST = True
except Exception:
    USE_ST = False

USE_TFIDF = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    USE_TFIDF = True
except Exception:
    USE_TFIDF = False

# ====== æ–‡æ¡£è¯»å–ï¼š.txt / .docxï¼ˆå¯é€‰ .doc via textractï¼‰======
def read_txt(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def read_docx(path: str) -> str:
    try:
        import docx  # python-docx
    except Exception:
        raise RuntimeError("è¯»å– .docx éœ€è¦ä¾èµ–: pip install python-docx")
    doc = docx.Document(path)
    return "\n".join(p.text for p in doc.paragraphs)

def read_doc(path: str) -> str:
    # å¯é€‰ï¼šå¦‚æœè£…äº† textractï¼Œå°±èƒ½è¯» .docï¼›æ²¡è£…å°±æç¤ºè½¬æˆ .docx
    try:
        import textract
    except Exception:
        raise RuntimeError("è¯»å– .doc éœ€è¦ textractï¼ˆæˆ–è¯·å…ˆè½¬æˆ .docxï¼‰ã€‚å®‰è£…ï¼špip install textract")
    return textract.process(path).decode("utf-8", "ignore")

def load_file(path: str) -> str:
    ext = os.path.splitext(path)[1].lower()
    if ext == ".txt":
        return read_txt(path)
    if ext == ".docx":
        return read_docx(path)
    if ext == ".doc":
        return read_doc(path)
    raise RuntimeError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}ï¼ˆæ”¯æŒ .txt / .docxï¼›.doc éœ€ textractï¼‰")

# ====== åˆ‡åˆ† ======
def chunk_text(text: str, chunk_size: int = 700, overlap: int = 120) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks, i, N = [], 0, len(text)
    while i < N:
        end = min(i + chunk_size, N)
        c = text[i:end].strip()
        if c:
            chunks.append(c)
        if end == N:
            break
        i = end - overlap if end - overlap > i else end
    return chunks

# ====== å‘é‡åç«¯ï¼ˆå›ºå®šï¼šç´¢å¼•ç”¨å“ªä¸ªï¼ŒæŸ¥è¯¢ä¹Ÿç”¨å“ªä¸ªï¼‰======
class EmbeddingBackend:
    def __init__(self, force_backend: str | None = None):
        self.backend = None
        self.vectorizer = None
        if force_backend == "st":
            from sentence_transformers import SentenceTransformer
            self.backend = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
            return
        if force_backend == "tfidf":
            from sklearn.feature_extraction.text import TfidfVectorizer
            self.vectorizer = TfidfVectorizer(max_features=50000)
            return
        # è‡ªåŠ¨é€‰æ‹©
        if USE_ST:
            self.backend = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        elif USE_TFIDF:
            self.vectorizer = TfidfVectorizer(max_features=50000)
        else:
            raise RuntimeError("ç¼ºå°‘å¯ç”¨çš„å‘é‡åŒ–åç«¯ï¼špip install sentence-transformers æˆ– scikit-learn")

    def fit_transform(self, texts: List[str]) -> np.ndarray:
        if self.backend is not None:
            embs = self.backend.encode(texts, normalize_embeddings=True)
            return np.asarray(embs, dtype=np.float32)
        X = self.vectorizer.fit_transform(texts)
        # L2 normalize
        norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12
        X = X.multiply(1.0 / norms[:, None])
        return np.asarray(X.todense(), dtype=np.float32)

    def transform(self, texts: List[str]) -> np.ndarray:
        if self.backend is not None:
            embs = self.backend.encode(texts, normalize_embeddings=True)
            return np.asarray(embs, dtype=np.float32)
        X = self.vectorizer.transform(texts)
        norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12
        X = X.multiply(1.0 / norms[:, None])
        return np.asarray(X.todense(), dtype=np.float32)

# ====== ç´¢å¼•ä¿å­˜/è¯»å– ======
def save_index(index_dir: str, chunks: List[str], embs: np.ndarray, meta: dict, sources: List[Dict]):
    os.makedirs(index_dir, exist_ok=True)
    with open(os.path.join(index_dir, "chunks.json"), "w", encoding="utf-8") as f:
        json.dump({"chunks": chunks, "sources": sources}, f, ensure_ascii=False)
    np.savez_compressed(
        os.path.join(index_dir, "embeddings.npz"),
        embeddings=embs.astype(np.float32),
        meta=np.bytes_(json.dumps(meta, ensure_ascii=False)),
    )

def load_index(index_dir: str):
    chunks_path = os.path.join(index_dir, "chunks.json")
    embs_path = os.path.join(index_dir, "embeddings.npz")
    if not (os.path.exists(chunks_path) and os.path.exists(embs_path)):
        raise FileNotFoundError(f"ç´¢å¼•ç¼ºå¤±ï¼š{chunks_path} æˆ– {embs_path}")
    with open(chunks_path, "r", encoding="utf-8") as f:
        j = json.load(f)
    chunks, sources = j["chunks"], j["sources"]
    data = np.load(embs_path)
    embs = data["embeddings"]
    meta = json.loads(str(data["meta"].tobytes(), "utf-8"))
    return chunks, sources, embs, meta

# ====== ç›¸ä¼¼åº¦æ£€ç´¢ ======
def top_k_cosine(q_vec: np.ndarray, db_vecs: np.ndarray, k: int = 3) -> List[int]:
    sims = (db_vecs @ q_vec.squeeze().astype(np.float32))
    return np.argsort(-sims)[:k].tolist()

# ====== ingest å‘½ä»¤ ======
def gather_files(inputs: List[str], use_glob: bool) -> List[str]:
    paths: List[str] = []
    for inp in inputs:
        if os.path.isdir(inp):
            for ext in ("*.txt", "*.docx", "*.doc"):
                paths.extend(glob.glob(os.path.join(inp, ext)))
        else:
            if use_glob and any(ch in inp for ch in ["*", "?", "["]):
                paths.extend(glob.glob(inp))
            else:
                paths.append(inp)
    # å»é‡ & å­˜åœ¨æ€§æ£€æŸ¥
    uniq = []
    for p in paths:
        p = os.path.abspath(p)
        if os.path.exists(p) and p not in uniq:
            uniq.append(p)
    return uniq

def cmd_ingest(args):
    files = gather_files(args.input, use_glob=args.glob)
    if not files:
        print("âŒ æœªå‘ç°å¯ç”¨æ–‡ä»¶ã€‚æ”¯æŒ .txt / .docxï¼ˆ.doc éœ€ textractï¼‰")
        return

    all_chunks: List[str] = []
    all_sources: List[Dict] = []
    for path in files:
        try:
            text = load_file(path)
        except Exception as e:
            print(f"è·³è¿‡ä¸å¯è¯»å–æ–‡ä»¶: {path} | åŸå› : {e}")
            continue
        chunks = chunk_text(text, chunk_size=args.chunk_size, overlap=args.overlap)
        for c in chunks:
            all_sources.append({"file": os.path.basename(path)})
        all_chunks.extend(chunks)

    if not all_chunks:
        print("âŒ æ–‡ä»¶ä¸ºç©ºæˆ–åˆ‡åˆ†åæ— å†…å®¹")
        return

    # é€‰æ‹©å¹¶å›ºåŒ–åç«¯
    backend_tag = "st" if USE_ST else ("tfidf" if USE_TFIDF else "none")
    eb = EmbeddingBackend(force_backend=backend_tag)
    embs = eb.fit_transform(all_chunks)

    meta = {
        "backend": backend_tag,  # 'st' / 'tfidf'
        "model": "all-MiniLM-L6-v2" if backend_tag == "st" else "tfidf",
        "chunk_size": args.chunk_size,
        "overlap": args.overlap,
        "num_chunks": len(all_chunks),
        "num_files": len(files),
        "files": [os.path.basename(p) for p in files],
    }
    save_index(args.index_dir, all_chunks, embs, meta, all_sources)
    print(f"âœ… ç´¢å¼•å®Œæˆï¼š{args.index_dir}")
    print(f"- æ–‡ä»¶æ•°: {meta['num_files']} -> {', '.join(meta['files'])}")
    print(f"- åˆ†ç‰‡æ•°: {meta['num_chunks']}")
    print(f"- å‘é‡ç»´åº¦: {embs.shape[1]}")
    print(f"- åç«¯: {meta['backend']} ({meta['model']})")

# ====== query å‘½ä»¤ ======
def cmd_query(args):
    chunks, sources, embs, meta = load_index(args.index_dir)
    backend_tag = meta.get("backend", "st")
    eb = EmbeddingBackend(force_backend=backend_tag)
    # TF-IDF éœ€è¦ç”¨è¯­æ–™æ‹Ÿåˆè¯è¡¨
    if backend_tag == "tfidf" and eb.vectorizer is not None:
        eb.vectorizer.fit(chunks)

    q_emb = eb.transform([args.q])
    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)

    idxs = top_k_cosine(q_emb, embs, k=args.k)
    print(f"ğŸ” Top-{args.k} ç»“æœï¼š\n")
    for rank, i in enumerate(idxs, 1):
        src = sources[i]["file"] if i < len(sources) else "unknown"
        preview = chunks[i][:400].replace("\n", " ")
        if len(chunks[i]) > 400: preview += "..."
        print(f"[{rank}] ç‰‡æ®µID={i} | æ¥è‡ª: {src}")
        print(preview)
        print("-" * 80)

# ====== CLI ======
def build_cli():
    p = argparse.ArgumentParser(description="å¤šæ–‡ä»¶æœ¬åœ°RAGï¼šè¯»å– .txt / .docxï¼ˆå¯é€‰ .docï¼‰ï¼Œåˆ‡åˆ†ã€å‘é‡åŒ–ã€æ£€ç´¢")
    sub = p.add_subparsers(dest="cmd")

    pi = sub.add_parser("ingest", help="æ„å»ºç´¢å¼•ï¼ˆæ”¯æŒå¤šæ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼‰")
    pi.add_argument("-i", "--input", nargs="+", required=True,
                    help="æ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼Œå¤šé¡¹ç”¨ç©ºæ ¼åˆ†éš”ã€‚ä¾‹å¦‚ï¼š-i a.txt b.docx ./docs '*.txt'")
    pi.add_argument("-o", "--index-dir", default="./kb_index", help="ç´¢å¼•è¾“å‡ºç›®å½•")
    pi.add_argument("--chunk-size", type=int, default=700, help="åˆ†ç‰‡å­—ç¬¦æ•°")
    pi.add_argument("--overlap", type=int, default=120, help="åˆ†ç‰‡é‡å å­—ç¬¦æ•°")
    pi.add_argument("--glob", action="store_true", help="æŠŠ -i å‚æ•°æŒ‰é€šé…ç¬¦å±•å¼€")
    pi.set_defaults(func=cmd_ingest)

    pq = sub.add_parser("query", help="æŸ¥è¯¢ç´¢å¼•")
    pq.add_argument("-o", "--index-dir", default="./kb_index", help="ç´¢å¼•ç›®å½•")
    pq.add_argument("-q", required=True, help="æŸ¥è¯¢é—®é¢˜")
    pq.add_argument("-k", type=int, default=3, help="è¿”å›ç‰‡æ®µæ•° Top-k")
    pq.set_defaults(func=cmd_query)

    return p

def main():
    parser = build_cli()
    args = parser.parse_args()
    if not hasattr(args, "func"):
        parser.print_help(); return
    args.func(args)

if __name__ == "__main__":
    main()
