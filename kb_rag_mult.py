#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, argparse, glob
from typing import List, Dict, Optional
import numpy as np

# ====== å¯é€‰åç«¯ï¼šSentence-Transformersï¼ˆä¼˜å…ˆï¼‰æˆ– TF-IDF ======
USE_ST = False
try:
    from sentence_transformers import SentenceTransformer
    USE_ST = True
except Exception:
    USE_ST = False

USE_TFIDF = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    USE_TFIDF = True
except Exception:
    USE_TFIDF = False

TFIDF_MODEL_FILENAME = "tfidf_vectorizer.joblib"  # æŒä¹…åŒ–æ–‡ä»¶å

# ====== æ–‡æ¡£è¯»å–ï¼š.txt / .docxï¼ˆå¯é€‰ .doc via textractï¼‰======
def read_txt(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def read_docx(path: str) -> str:
    try:
        import docx  # python-docx
    except Exception:
        raise RuntimeError("è¯»å– .docx éœ€è¦ä¾èµ–: pip install python-docx")
    doc = docx.Document(path)
    return "\n".join(p.text for p in doc.paragraphs)

def read_doc(path: str) -> str:
    # å¯é€‰ï¼šå¦‚æœè£…äº† textractï¼Œå°±èƒ½è¯» .docï¼›æ²¡è£…å°±æç¤ºè½¬æˆ .docx
    try:
        import textract
    except Exception:
        raise RuntimeError("è¯»å– .doc éœ€è¦ textractï¼ˆæˆ–è¯·å…ˆè½¬æˆ .docxï¼‰ã€‚å®‰è£…ï¼špip install textract")
    return textract.process(path).decode("utf-8", "ignore")

def load_file(path: str) -> str:
    ext = os.path.splitext(path)[1].lower()
    if ext == ".txt":
        return read_txt(path)
    if ext == ".docx":
        return read_docx(path)
    if ext == ".doc":
        return read_doc(path)
    raise RuntimeError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}ï¼ˆæ”¯æŒ .txt / .docxï¼›.doc éœ€ textractï¼‰")

# ====== åˆ‡åˆ† ======
def chunk_text(text: str, chunk_size: int = 700, overlap: int = 120) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks, i, N = [], 0, len(text)
    while i < N:
        end = min(i + chunk_size, N)
        c = text[i:end].strip()
        if c:
            chunks.append(c)
        if end == N:
            break
        i = end - overlap if end - overlap > i else end
    return chunks

# ====== å‘é‡åç«¯ï¼ˆç´¢å¼•/æŸ¥è¯¢ä¸€è‡´ï¼‰======
class EmbeddingBackend:
    """
    - å½“ force_backend == "st"ï¼šå¼ºåˆ¶ ST
    - å½“ force_backend == "tfidf"ï¼šå¼ºåˆ¶ TF-IDF
    - å…¶ä»–ï¼šä¼˜å…ˆ STï¼Œå¤±è´¥åˆ™å›é€€ TF-IDFï¼ˆè‹¥å¯ç”¨ï¼‰
    - ST æ¨¡å‹è·¯å¾„ä¼˜å…ˆï¼šä» EMB_MODEL è¯»ï¼Œå¦‚æœæ˜¯ç›®å½• â†’ ç›´æ¥åŠ è½½æœ¬åœ°ç›®å½•ï¼›å¦åˆ™å½“ä½œ repo_id
    - ç¦»çº¿ï¼šTRANSFORMERS_OFFLINE / HF_HUB_OFFLINE ç”Ÿæ•ˆ
    """
    def __init__(self, force_backend: Optional[str] = None, tfidf_model_path: Optional[str] = None):
        self.backend = None
        self.vectorizer: Optional[TfidfVectorizer] = None

        offline = os.getenv("TRANSFORMERS_OFFLINE", "").lower() in ("1", "true", "yes")
        if offline:
            os.environ.setdefault("HF_HUB_OFFLINE", "1")

        def _load_st():
            model_name = os.getenv("EMB_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
            # ç›®å½•ä¼˜å…ˆ
            if os.path.isdir(model_name):
                print({"embedding_backend": "st", "load": "local_dir", "path": model_name, "offline": offline})
                return SentenceTransformer(model_name)
            else:
                print({"embedding_backend": "st", "load": "repo_id", "repo": model_name, "offline": offline})
                return SentenceTransformer(model_name)

        def _load_tfidf():
            print({"embedding_backend": "tfidf", "model_file": tfidf_model_path})
            if tfidf_model_path and os.path.isfile(tfidf_model_path):
                try:
                    import joblib
                    vec = joblib.load(tfidf_model_path)
                    return vec
                except Exception as e:
                    print({"tfidf_load_failed": str(e)})
            # fallbackï¼šä¸´æ—¶æ–°å»ºä¸€ä¸ªï¼ˆå¯èƒ½éœ€è¦ fitï¼‰
            return TfidfVectorizer(max_features=50000)

        # é€‰æ‹©ç­–ç•¥
        if force_backend == "st":
            if not USE_ST:
                raise RuntimeError("åç«¯æŒ‡å®šä¸º STï¼Œä½†æœªå®‰è£… sentence-transformers")
            self.backend = _load_st()
            return

        if force_backend == "tfidf":
            if not USE_TFIDF:
                raise RuntimeError("åç«¯æŒ‡å®šä¸º TF-IDFï¼Œä½†æœªå®‰è£… scikit-learn")
            self.vectorizer = _load_tfidf()
            return

        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆ STï¼Œå¤±è´¥å›é€€ TF-IDF
        if USE_ST:
            try:
                self.backend = _load_st()
                return
            except Exception as e:
                print({"st_init_failed": str(e), "fallback": "tfidf"})
        if USE_TFIDF:
            self.vectorizer = _load_tfidf()
            return
        raise RuntimeError("ç¼ºå°‘å¯ç”¨çš„å‘é‡åŒ–åç«¯ï¼šè¯·å®‰è£… sentence-transformers æˆ– scikit-learn")

    def fit_transform(self, texts: List[str]) -> np.ndarray:
        if self.backend is not None:
            embs = self.backend.encode(texts, normalize_embeddings=True)
            return np.asarray(embs, dtype=np.float32)
        X = self.vectorizer.fit_transform(texts)
        # L2 normalize
        norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12
        X = X.multiply(1.0 / norms[:, None])
        return np.asarray(X.todense(), dtype=np.float32)

    def transform(self, texts: List[str]) -> np.ndarray:
        if self.backend is not None:
            embs = self.backend.encode(texts, normalize_embeddings=True)
            return np.asarray(embs, dtype=np.float32)
        X = self.vectorizer.transform(texts)
        norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12
        X = X.multiply(1.0 / norms[:, None])
        return np.asarray(X.todense(), dtype=np.float32)

# ====== ç´¢å¼•ä¿å­˜/è¯»å– ======
def save_index(index_dir: str, chunks: List[str], embs: np.ndarray, meta: dict, sources: List[Dict], tfidf_vec: Optional[TfidfVectorizer] = None):
    os.makedirs(index_dir, exist_ok=True)
    with open(os.path.join(index_dir, "chunks.json"), "w", encoding="utf-8") as f:
        json.dump({"chunks": chunks, "sources": sources}, f, ensure_ascii=False)
    np.savez_compressed(
        os.path.join(index_dir, "embeddings.npz"),
        embeddings=embs.astype(np.float32),
        meta=np.bytes_(json.dumps(meta, ensure_ascii=False)),
    )
    # æŒä¹…åŒ– TF-IDFï¼ˆå¦‚ä½¿ç”¨ï¼‰
    if tfidf_vec is not None:
        try:
            import joblib
            joblib.dump(tfidf_vec, os.path.join(index_dir, TFIDF_MODEL_FILENAME))
        except Exception as e:
            print({"warn": "ä¿å­˜ TF-IDF æ¨¡å‹å¤±è´¥", "err": str(e)})

def load_index(index_dir: str):
    chunks_path = os.path.join(index_dir, "chunks.json")
    embs_path = os.path.join(index_dir, "embeddings.npz")
    if not (os.path.exists(chunks_path) and os.path.exists(embs_path)):
        raise FileNotFoundError(f"ç´¢å¼•ç¼ºå¤±ï¼š{chunks_path} æˆ– {embs_path}")
    with open(chunks_path, "r", encoding="utf-8") as f:
        j = json.load(f)
    chunks, sources = j["chunks"], j["sources"]
    data = np.load(embs_path)
    embs = data["embeddings"]
    meta = json.loads(str(data["meta"].tobytes(), "utf-8"))
    return chunks, sources, embs, meta

# ====== ç›¸ä¼¼åº¦æ£€ç´¢ ======
def top_k_cosine(q_vec: np.ndarray, db_vecs: np.ndarray, k: int = 3) -> List[int]:
    sims = (db_vecs @ q_vec.squeeze().astype(np.float32))
    return np.argsort(-sims)[:k].tolist()

# ====== ingest å‘½ä»¤ ======
# æ’é™¤çš„æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸åº”è¢«å½“ä½œçŸ¥è¯†åº“çš„æ–‡ä»¶ï¼‰
EXCLUDED_FILES = {
    "requirements.txt",
    "requirements-dev.txt",
    "requirements_dev.txt",
    "dev-requirements.txt",
    "test-requirements.txt",
    "README.txt",
    "LICENSE.txt",
    "CHANGELOG.txt",
    "TODO.txt",
    ".env.txt",
}

def gather_files(inputs: List[str], use_glob: bool) -> List[str]:
    paths: List[str] = []
    for inp in inputs:
        if os.path.isdir(inp):
            for ext in ("*.txt", "*.docx", "*.doc"):
                for f in glob.glob(os.path.join(inp, ext)):
                    # æ’é™¤ä¸åº”è¢«å½“ä½œçŸ¥è¯†åº“çš„æ–‡ä»¶
                    if os.path.basename(f).lower() not in {x.lower() for x in EXCLUDED_FILES}:
                        paths.append(f)
        else:
            if use_glob and any(ch in inp for ch in ["*", "?", "["]):
                paths.extend(glob.glob(inp))
            else:
                paths.append(inp)
    # å»é‡ & å­˜åœ¨æ€§æ£€æŸ¥
    uniq = []
    for p in paths:
        p = os.path.abspath(p)
        if os.path.exists(p) and p not in uniq:
            uniq.append(p)
    return uniq

def cmd_ingest(args):
    files = gather_files(args.input, use_glob=args.glob)
    if not files:
        print("âŒ æœªå‘ç°å¯ç”¨æ–‡ä»¶ã€‚æ”¯æŒ .txt / .docxï¼ˆ.doc éœ€ textractï¼‰")
        return

    all_chunks: List[str] = []
    all_sources: List[Dict] = []
    for path in files:
        try:
            text = load_file(path)
        except Exception as e:
            print(f"è·³è¿‡ä¸å¯è¯»å–æ–‡ä»¶: {path} | åŸå› : {e}")
            continue
        chunks = chunk_text(text, chunk_size=args.chunk_size, overlap=args.overlap)
        for _ in chunks:
            all_sources.append({"file": os.path.basename(path)})
        all_chunks.extend(chunks)

    if not all_chunks:
        print("âŒ æ–‡ä»¶ä¸ºç©ºæˆ–åˆ‡åˆ†åæ— å†…å®¹")
        return

    # é€‰æ‹©å¹¶å›ºåŒ–åç«¯
    backend_tag = "st" if USE_ST else ("tfidf" if USE_TFIDF else "none")
    tfidf_model_path = os.path.join(args.index_dir, TFIDF_MODEL_FILENAME) if backend_tag == "tfidf" else None
    eb = EmbeddingBackend(force_backend=backend_tag, tfidf_model_path=tfidf_model_path)
    embs = eb.fit_transform(all_chunks)

    meta = {
        "backend": backend_tag,  # 'st' / 'tfidf'
        "model": "all-MiniLM-L6-v2" if backend_tag == "st" else "tfidf",
        "chunk_size": args.chunk_size,
        "overlap": args.overlap,
        "num_chunks": len(all_chunks),
        "num_files": len(files),
        "files": [os.path.basename(p) for p in files],
    }
    save_index(args.index_dir, all_chunks, embs, meta, all_sources, tfidf_vec=eb.vectorizer if backend_tag == "tfidf" else None)

    print(f"âœ… ç´¢å¼•å®Œæˆï¼š{args.index_dir}")
    print(f"- æ–‡ä»¶æ•°: {meta['num_files']} -> {', '.join(meta['files'])}")
    print(f"- åˆ†ç‰‡æ•°: {meta['num_chunks']}")
    print(f"- å‘é‡ç»´åº¦: {embs.shape[1]}")
    print(f"- åç«¯: {meta['backend']} ({meta['model']})")
    if backend_tag == "tfidf":
        print(f"- TF-IDF æ¨¡å‹: {TFIDF_MODEL_FILENAME}")

# ====== query å‘½ä»¤ ======
def cmd_query(args):
    chunks, sources, embs, meta = load_index(args.index_dir)
    backend_tag = meta.get("backend", "st")

    tfidf_model_path = os.path.join(args.index_dir, TFIDF_MODEL_FILENAME) if backend_tag == "tfidf" else None
    eb = EmbeddingBackend(force_backend=backend_tag, tfidf_model_path=tfidf_model_path)

    # æ³¨æ„ï¼šç°åœ¨ä¸å†å¯¹ TF-IDF è¿›è¡Œ fit() â€”â€” ç›´æ¥åŠ è½½æŒä¹…åŒ–å‘é‡å™¨
    if backend_tag == "tfidf" and (eb.vectorizer is None):
        raise RuntimeError("TF-IDF å‘é‡å™¨æœªåŠ è½½ã€‚è¯·é‡æ–° ingest æˆ–æ£€æŸ¥æŒä¹…åŒ–æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")

    q_emb = eb.transform([args.q])
    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)

    idxs = top_k_cosine(q_emb, embs, k=args.k)
    print(f"ğŸ” Top-{args.k} ç»“æœï¼š\n")
    for rank, i in enumerate(idxs, 1):
        src = sources[i]["file"] if i < len(sources) else "unknown"
        preview = chunks[i][:400].replace("\n", " ")
        if len(chunks[i]) > 400: preview += "..."
        print(f"[{rank}] ç‰‡æ®µID={i} | æ¥è‡ª: {src}")
        print(preview)
        print("-" * 80)

# ====== CLI ======
def build_cli():
    p = argparse.ArgumentParser(description="å¤šæ–‡ä»¶æœ¬åœ°RAGï¼šè¯»å– .txt / .docxï¼ˆå¯é€‰ .docï¼‰ï¼Œåˆ‡åˆ†ã€å‘é‡åŒ–ã€æ£€ç´¢")
    sub = p.add_subparsers(dest="cmd")

    pi = sub.add_parser("ingest", help="æ„å»ºç´¢å¼•ï¼ˆæ”¯æŒå¤šæ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼‰")
    pi.add_argument("-i", "--input", nargs="+", required=True,
                    help="æ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼Œå¤šé¡¹ç”¨ç©ºæ ¼åˆ†éš”ã€‚ä¾‹å¦‚ï¼š-i a.txt b.docx ./docs '*.txt'")
    pi.add_argument("-o", "--index-dir", default="./kb_index", help="ç´¢å¼•è¾“å‡ºç›®å½•")
    pi.add_argument("--chunk-size", type=int, default=700, help="åˆ†ç‰‡å­—ç¬¦æ•°")
    pi.add_argument("--overlap", type=int, default=120, help="åˆ†ç‰‡é‡å å­—ç¬¦æ•°")
    pi.add_argument("--glob", action="store_true", help="æŠŠ -i å‚æ•°æŒ‰é€šé…ç¬¦å±•å¼€")
    pi.set_defaults(func=cmd_ingest)

    pq = sub.add_parser("query", help="æŸ¥è¯¢ç´¢å¼•")
    pq.add_argument("-o", "--index-dir", default="./kb_index", help="ç´¢å¼•ç›®å½•")
    pq.add_argument("-q", required=True, help="æŸ¥è¯¢é—®é¢˜")
    pq.add_argument("-k", type=int, default=3, help="è¿”å›ç‰‡æ®µæ•° Top-k")
    pq.set_defaults(func=cmd_query)

    return p

def main():
    parser = build_cli()
    args = parser.parse_args()
    if not hasattr(args, "func"):
        parser.print_help(); return
    args.func(args)

if __name__ == "__main__":
    main()
